<!DOCTYPE html>
<html>
<head>
    <title>Zero-Config Face Recognition</title>
    <script src="https://cdn.jsdelivr.net/npm/@vladmandic/face-api@1.7.0/dist/face-api.min.js"></script>
    <style>
        :root { --bg: #0a0a0a; --fg: #f0f0f0; --primary: #00ff88; --secondary: #222222; }
        body { margin: 0; background: var(--bg); color: var(--fg); font-family: Arial, sans-serif; }
        #container { display: flex; flex-direction: column; align-items: center; padding: 20px; }
        #video { width: 640px; height: 480px; border-radius: 8px; background: #000; }
        .status { margin-top: 20px; padding: 15px; background: var(--secondary); border-radius: 8px; width: 640px; }
    </style>
</head>
<body>
    <div id="container">
        <video id="video" autoplay muted></video>
        <div class="status" id="status">Initializing system...</div>
    </div>

<script>
(async () => {
    const status = document.getElementById('status');
    const video = document.getElementById('video');
    let faceMatcher = null;
    const knownFaces = new Map();
    let isTraining = false;
    let currentUser = 1;

    // Load models from CDN
    try {
        status.textContent = 'Loading neural networks...';
        await faceapi.nets.tinyFaceDetector.loadFromUri('https://cdn.jsdelivr.net/npm/@vladmandic/face-api@1.7.0/model/');
        await faceapi.nets.faceLandmark68Net.loadFromUri('https://cdn.jsdelivr.net/npm/@vladmandic/face-api@1.7.0/model/');
        await faceapi.nets.faceRecognitionNet.loadFromUri('https://cdn.jsdelivr.net/npm/@vladmandic/face-api@1.7.0/model/');
    } catch (error) {
        status.textContent = 'Failed to load AI models. Please check internet connection.';
        return;
    }

    // Setup camera
    try {
        const stream = await navigator.mediaDevices.getUserMedia({ video: {} });
        video.srcObject = stream;
        await new Promise(resolve => video.onloadedmetadata = resolve);
        video.play();
    } catch (error) {
        status.textContent = 'Camera access required. Please enable permissions.';
        return;
    }

    // Auto-enrollment process
    const captureTrainingData = async () => {
        status.textContent = `Training User ${currentUser}: Face the camera (${knownFaces.size}/5 samples)`;
        
        try {
            const result = await faceapi.detectSingleFace(video, new faceapi.TinyFaceDetectorOptions())
                .withFaceLandmarks()
                .withFaceDescriptor();
            
            if (result) {
                const descriptors = knownFaces.get(`User ${currentUser}`) || [];
                descriptors.push(result.descriptor);
                knownFaces.set(`User ${currentUser}`, descriptors);
            }

            if (knownFaces.size < 5) {
                setTimeout(captureTrainingData, 500);
            } else {
                finalizeTraining();
            }
        } catch (error) {
            console.error('Detection error:', error);
            setTimeout(captureTrainingData, 500);
        }
    };

    const finalizeTraining = () => {
        const labeledDescriptors = Array.from(knownFaces.entries()).map(([label, descriptors]) => 
            new faceapi.LabeledFaceDescriptors(label, descriptors)
        );
        
        faceMatcher = new faceapi.FaceMatcher(labeledDescriptors, 0.6);
        status.textContent = `User ${currentUser} trained! Next user in 3 seconds...`;
        
        if (currentUser < 10) {
            currentUser++;
            knownFaces.clear();
            setTimeout(captureTrainingData, 3000);
        } else {
            status.textContent = 'Training complete! Starting recognition...';
            startRecognition();
        }
    };

    // Recognition loop
    const startRecognition = async () => {
        setInterval(async () => {
            const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
                .withFaceLandmarks()
                .withFaceDescriptors();
            
            const results = detections.map(d => 
                faceMatcher.findBestMatch(d.descriptor)
            );
            
            status.textContent = results.length > 0 
                ? results.map(r => `${r.label} (${Math.round((1 - r.distance) * 100)}%)`).join('\n')
                : 'No faces detected';
        }, 1000);
    };

    // Start automatic training
    video.addEventListener('play', () => {
        faceapi.matchDimensions(video, video);
        captureTrainingData();
    });
})();
</script>
</body>
</html>
